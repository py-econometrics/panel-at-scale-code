{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%load_ext watermark\n",
    "%watermark --iversions\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "import io\n",
    "from utils.benchmark import Bench\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from utils.dgps import generate_benchmark_data\n",
    "from utils.estimators import (\n",
    "    twfe_fixest, \n",
    "    twfe_fixest_compressed, \n",
    "    twfe_statsmodels, \n",
    "    duck_mundlak, \n",
    "    event_study_fixest, \n",
    "    duck_mundlak_event, \n",
    "    event_study_statsmodels\n",
    ")\n",
    "from utils.benchmark import Bench\n",
    "from itertools import product\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7ad3aceb06946b58820e7245ef9b16c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing benchmarks:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded into DuckDB database: benchmarks.db\n",
      "Data loaded into DuckDB database: benchmarks.db\n"
     ]
    }
   ],
   "source": [
    "N_values = [10_000, 100_000]\n",
    "T_values = [14, 28, 42]\n",
    "models = [\n",
    "    twfe_statsmodels, \n",
    "    twfe_fixest, \n",
    "    twfe_fixest_compressed, \n",
    "    event_study_statsmodels, \n",
    "    event_study_fixest, \n",
    "    duck_mundlak, \n",
    "    duck_mundlak_event\n",
    "]\n",
    "T0_values = [7]\n",
    "iter = 3\n",
    "\n",
    "all_benchmarks_df = pd.DataFrame()\n",
    "\n",
    "# Initialize the tqdm progress bar for the outer loop\n",
    "with tqdm(total= len(models) * len(N_values) * len(T_values) * len(T0_values), desc=\"Processing benchmarks\") as pbar:\n",
    "\n",
    "    for model, N, T, T0 in product(models, N_values, T_values, T0_values):\n",
    "\n",
    "        print(\"Model:\", model)\n",
    "        bench = Bench(N=N, T=T, T0=T0, iter=iter)\n",
    "\n",
    "        # Suppress tqdm output in called functions for Jupyter\n",
    "        temp_stdout = io.StringIO()\n",
    "        old_stdout = sys.stdout\n",
    "        old_stderr = sys.stderr\n",
    "        sys.stdout = temp_stdout\n",
    "        sys.stderr = temp_stdout\n",
    "        try:\n",
    "            bench.mark(fun=model)\n",
    "        finally:\n",
    "            sys.stdout = old_stdout\n",
    "            sys.stderr = old_stderr\n",
    "\n",
    "        # Update the progress bar manually\n",
    "        pbar.update(1)\n",
    "\n",
    "        # Collect results\n",
    "        bench_df = bench.to_dataframe()\n",
    "        bench_df[\"N\"] = N\n",
    "        bench_df[\"T\"] = T\n",
    "        bench_df[\"T0\"] = T0\n",
    "\n",
    "        bench_df.to_csv(f\"data/benchmark_{model}_{N}_{T}_{T0}.csv\")\n",
    "\n",
    "        all_benchmarks_df = pd.concat(\n",
    "            [all_benchmarks_df, bench_df], \n",
    "            axis=0\n",
    "        )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "all_files = glob.glob(os.path.join(\"data\", \"*.csv\"))\n",
    "all_benchmarks_list = [pd.read_csv(file) for file in all_files]\n",
    "\n",
    "all_benchmarks_df = pd.concat(all_benchmarks_list, axis = 0)\n",
    "\n",
    "models = [\n",
    "    \"duck_mundlak\", \n",
    "    \"twfe_fixest\", \n",
    "    \"twfe_statsmodels\", \n",
    "    \"twfe_fixest_compressed\",\n",
    "    \"event_study_fixest\", \n",
    "    \"duck_mundlak_event\", \n",
    "    \"event_study_statsmodels\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_benchmarks_df_melted = all_benchmarks_df.melt(\n",
    "    id_vars=['N', 'T', 'T0'],\n",
    "    value_vars=models, \n",
    "    var_name='model',\n",
    "    value_name='value'\n",
    ")\n",
    "\n",
    "median_run = (\n",
    "    all_benchmarks_df_melted\n",
    "        .groupby([\"N\", \"T\", \"T0\", \"model\"])\n",
    "        .median()\n",
    "        .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_run_pooled = median_run[median_run.model.isin([\n",
    "    \"duck_mundlak\", \n",
    "    \"twfe_fixest\", \n",
    "    \"twfe_statsmodels\", \n",
    "    \"twfe_fixest_compressed\"\n",
    "])]\n",
    "\n",
    "median_run_event = median_run[median_run.model.isin([\n",
    "    \"event_study_fixest\", \n",
    "    \"duck_mundlak_event\", \n",
    "    \"event_study_statsmodels\"\n",
    "])]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Pooled Effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(median_run_pooled, col=\"T\", row=\"N\", margin_titles=True, height=4, aspect=1)\n",
    "g.map_dataframe(sns.barplot, x='model', y='value', hue='model', palette='Set2', legend=False)\n",
    "\n",
    "# Adjust the x-axis labels' rotation for each subplot\n",
    "for ax in g.axes.flat:\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# Adjust layout\n",
    "plt.yscale('log')\n",
    "plt.subplots_adjust(top=0.9)\n",
    "g.fig.suptitle('Benchmarks of Panel Implementations')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Event Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(median_run_event, col=\"T\", row=\"N\", margin_titles=True, height=4, aspect=1)\n",
    "g.map_dataframe(sns.barplot, x='model', y='value', hue='model', palette='Set2', legend=False)\n",
    "\n",
    "# Adjust the x-axis labels' rotation for each subplot\n",
    "for ax in g.axes.flat:\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# Adjust layout\n",
    "plt.yscale('log')\n",
    "plt.subplots_adjust(top=0.9)\n",
    "g.fig.suptitle('Benchmarks of Panel Implementations')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
